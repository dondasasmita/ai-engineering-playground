{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20ce7ddf-1d75-4b14-b00e-c3524e7d32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install pyspark using pip\n",
    "!pip install --ignore-install -q pyspark\n",
    "# install findspark using pip\n",
    "!pip install --ignore-install -q findspark\n",
    "\n",
    "#from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import collections\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Ingestion\").config('spark.ui.port', '4050').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf6506d-54ed-4063-af62-2addee70d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file people.csv\n",
    "df = spark.read.format('csv') \\\n",
    "                .option(\"inferSchema\",\"true\") \\\n",
    "                .option(\"header\",\"true\") \\\n",
    "                .option(\"sep\",\";\") \\\n",
    "                .load(\"people.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7e14d95-3ebc-47ba-b41f-cefbfe241874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d68b979-cfca-4211-95c5-a745d585dfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efac8a99-1209-49c7-bd56-13a21f05bff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n",
      "| name|age|        job|\n",
      "+-----+---+-----------+\n",
      "|Jorge| 30|  Developer|\n",
      "|  Bob| 32|  Developer|\n",
      "| Zoey| 31|AI Engineer|\n",
      "+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# assuming df is your DataFrame\n",
    "\n",
    "new_row = Row(name=\"Zoey\", age=31, job=\"AI Engineer\")\n",
    "new_df = df.union(df.sparkSession.createDataFrame([new_row]))\n",
    "\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253f5912-c336-44c0-98bc-6780e049f22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.csv(\"people-v2.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a024c0f3-80ca-4c92-b3a4-e1d4bf8ed740",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = spark.read.csv(\"people-v2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3809568f-83f4-40ef-8ed9-b80c60361060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----------+\n",
      "| name|age|        job|\n",
      "+-----+---+-----------+\n",
      "|Jorge| 30|  Developer|\n",
      "|  Bob| 32|  Developer|\n",
      "| Zoey| 31|AI Engineer|\n",
      "+-----+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801579d-0aa0-44fe-b63d-ee4a8f5799b2",
   "metadata": {},
   "source": [
    "## Read JSON\n",
    "There are two methods we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86430ebe-fbd5-4d39-8370-9a13dd970596",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Method 1 ###\n",
    "# df_3 = spark.read.format('json') \\\n",
    "#                 .option(\"inferSchema\",\"true\") \\\n",
    "#                 .load(\"article.json\")\n",
    "\n",
    "### Method 2 ###\n",
    "articleDF = spark.read.json(\"article.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5097588a-1d6f-4095-8913-619e90cbb515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "articleDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ad6b1cb-a98d-4126-8c91-2ed3e1c936c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------+-------+--------------------+--------------------+\n",
      "|                body|          final_tldr|            headline|isPublished|    nid|  summarization_jobs|                tldr|\n",
      "+--------------------+--------------------+--------------------+-----------+-------+--------------------+--------------------+\n",
      "| <p>COPENHAGEN: A...|<ul>\\r\\n\\t<li>As ...|European countrie...|       true|4487741|{123 -> {start_ti...|[{start_time -> 2...|\n",
      "+--------------------+--------------------+--------------------+-----------+-------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Need to parse the json file into python dictionary before creating a dataframe\n",
    "import json\n",
    "\n",
    "with open(\"article.json\", \"r\") as file:\n",
    "    my_json_file = json.load(file)\n",
    "\n",
    "articleDF = spark.createDataFrame([my_json_file])\n",
    "\n",
    "# Show the DataFrame\n",
    "articleDF.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "289f6db4-fe9b-4c47-9ea4-22c9c683d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|  summarization_jobs|                tldr|\n",
      "+--------------------+--------------------+\n",
      "|{123 -> {start_ti...|[{start_time -> 2...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "articleDF.select(\"summarization_jobs\", \"tldr\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43867b2c-cc24-4195-ab4c-e4f1102835af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|summarization_jobs_entries|\n",
      "+--------------------------+\n",
      "|      [{123, {start_tim...|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_entries\n",
    "\n",
    "articleDF.select(map_entries(articleDF.summarization_jobs).alias(\"summarization_jobs_entries\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93cd69ad-6ab3-41fa-85ea-33df810ae6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|       tldr_elements|\n",
      "+--------------------+\n",
      "|{start_time -> 20...|\n",
      "|{start_time -> 20...|\n",
      "|{start_time -> 20...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "articleDF.select(explode(articleDF.tldr).alias(\"tldr_elements\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260cf48-0d2f-4f29-ac1b-f7a833da37eb",
   "metadata": {},
   "source": [
    "## Parquet\n",
    "\n",
    "### What is Parquet?\n",
    "Parquet is a columnar storage file format optimized for analytical queries. It stores data by columns rather than rows, which allows for efficient data compression and faster query performance, especially when only a subset of columns is needed.\n",
    "\n",
    "### Who should use Parquet?\n",
    "Data engineers, data scientists, and analysts working with large datasets and requiring efficient storage and retrieval for analytical queries should consider using Parquet. It is particularly useful in big data environments and data lakes.\n",
    "\n",
    "### When should you use Parquet?\n",
    "- Large Datasets: When working with very large datasets that need to be stored and queried efficiently.\n",
    "- Columnar Queries: When your queries frequently target specific columns rather than entire rows.\n",
    "- Data Lakes: When storing data in a data lake for use by multiple analytics services.\n",
    "- Schema Evolution: When you need flexibility to evolve the schema over time without significant overhead.\n",
    "\n",
    "### Where is Parquet implemented?\n",
    "Parquet is commonly used in cloud storage solutions (e.g., AWS S3, Google Cloud Storage, Azure Data Lake Storage), Hadoop Distributed File System (HDFS), and local file systems. It integrates well with various big data processing frameworks like Apache Spark, Hive, and Presto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ee79806e-fd4f-48d8-92ed-efc505fe5909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames can be saved as Parquet files, maintaining the schema information.\n",
    "articleDF.write.format(\"parquet\").mode(\"overwrite\").save(\"article.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90660ee7-6b1a-410c-b882-5dfc92ebc921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    nid|\n",
      "+-------+\n",
      "|4487741|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in the Parquet file created above.\n",
    "# Parquet files are self-describing so the schema is preserved.\n",
    "# The result of loading a parquet file is also a DataFrame.\n",
    "parquetFile = spark.read.parquet(\"article.parquet\")\n",
    "# Parquet files can also be used to create a temporary view and then used in SQL statements.\n",
    "parquetFile.createOrReplaceTempView(\"parquetFile\")\n",
    "published_articles = spark.sql(\"SELECT nid FROM parquetFile WHERE isPublished = true\")\n",
    "published_articles.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60d360-1762-4ebc-988b-c7c8d321487c",
   "metadata": {},
   "source": [
    "## What is PyArrow?\n",
    "PyArrow is the Python implementation of Apache Arrow, a cross-language development platform for in-memory data. Apache Arrow provides a standardized columnar memory format optimized for analytical operations and efficient data interchange between different data processing frameworks. PyArrow offers Python bindings to the Arrow C++ libraries, enabling efficient data manipulation and interoperability with other big data tools.\n",
    "\n",
    "### Who uses PyArrow?\n",
    "PyArrow is primarily used by data scientists, data engineers, and developers who work with large datasets and require efficient data processing and interchange. It is particularly beneficial for those using Python-based data analysis libraries like NumPy and pandas, and for those working within big data ecosystems involving tools like Apache Spark.\n",
    "\n",
    "### When should you use PyArrow?\n",
    "PyArrow should be used in scenarios that involve:\n",
    "- Large Datasets: When handling large volumes of data that need efficient storage and fast access.\n",
    "- Data Interchange: When there is a need to transfer data efficiently between different systems or languages, such as between Python and JVM-based systems.\n",
    "- Performance Optimization: When optimizing the performance of data processing tasks, especially in environments where pandas and NumPy are heavily used.\n",
    "- In-Memory Analytics: When performing in-memory data analytics that benefit from a columnar data format.\n",
    "\n",
    "### Where is PyArrow used?\n",
    "PyArrow is used in various environments:\n",
    "- Big Data Platforms: Integrated with tools like Apache Spark, Hadoop, and cloud data platforms (e.g., AWS S3, Google Cloud Storage, Azure Data Lake Storage).\n",
    "- Local Systems: For in-memory data processing on local machines using Python.\n",
    "-  Data Lakes: For efficient storage and retrieval of large datasets in data lakes.\n",
    "- Cloud Environments: For data processing and analytics in cloud-based systems.\n",
    "\n",
    "### Why use PyArrow?\n",
    "There are several compelling reasons to use PyArrow:\n",
    "- Performance: PyArrow provides significant performance improvements for data processing tasks by leveraging a columnar memory format and efficient data serialization.\n",
    "- Interoperability: It enables seamless data interchange between different languages and frameworks, such as between Python (pandas, NumPy) and JVM-based systems (Spark).\n",
    "- Memory Efficiency: By using a columnar format, PyArrow optimizes memory usage, which is particularly beneficial for large-scale data analytics.\n",
    "- Integration: PyArrow integrates well with existing Python data analysis libraries and big data tools, making it easy to incorporate into existing workflows.\n",
    "- Standardization: It provides a standardized way to handle in-memory data, facilitating better collaboration and consistency across different tools and platforms.\n",
    "\n",
    "### How does PyArrow work?\n",
    "PyArrow works by providing Python bindings to the Arrow C++ libraries, enabling efficient data manipulation and interchange. Here are some key functionalities:\n",
    "\n",
    "- Columnar Memory Format: PyArrow uses Apache Arrow's columnar memory format, which organizes data by columns rather than rows, allowing for efficient compression and fast columnar access.\n",
    "- Data Interchange: It facilitates efficient data transfer between Python and other systems by using Arrow's standardized format, reducing serialization overhead.\n",
    "- Integration with Pandas: PyArrow can be used as a backend for pandas, allowing pandas DataFrames to store data in Arrow format, improving performance for certain operations.\n",
    "- Support for Various File Systems: PyArrow supports reading and writing data to various file systems, including local storage, HDFS, and cloud storage like AWS S3.\n",
    "- Optimized Data Processing: PyArrow can be used to optimize data processing tasks in Spark by enabling efficient data transfer between Python and JVM processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15753f8b-8bb5-4dcf-a1e0-597c69f664ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.FileMetaData object at 0xffff5974c720>\n",
      "  created_by: parquet-cpp-arrow version 13.0.0\n",
      "  num_columns: 10\n",
      "  num_rows: 52203\n",
      "  num_row_groups: 1\n",
      "  format_version: 2.6\n",
      "  serialized_size: 2079\n",
      "<pyarrow._parquet.RowGroupMetaData object at 0xffff5973c2c0>\n",
      "  num_columns: 10\n",
      "  num_rows: 52203\n",
      "  total_byte_size: 431095\n",
      "<pyarrow._parquet.Statistics object at 0xffff5991b790>\n",
      "  has_min_max: True\n",
      "  min: 195000.0\n",
      "  max: 1088888.0\n",
      "  null_count: 0\n",
      "  distinct_count: 0\n",
      "  num_values: 52203\n",
      "  physical_type: DOUBLE\n",
      "  logical_type: None\n",
      "  converted_type (legacy): NONE\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "hdb_table = pv.read_csv(\"resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.csv\")\n",
    "\n",
    "pq.write_table(hdb_table,'resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')\n",
    "\n",
    "hdb_parquet = pq.ParquetFile('resale-flat-prices-based-on-registration-date-from-mar-2012-to-dec-2014.parquet')\n",
    "\n",
    "print(hdb_parquet.metadata)\n",
    "\n",
    "# inspect the parquet row group metadata\n",
    "print(hdb_parquet.metadata.row_group(0))\n",
    "\n",
    "# inspect the column chunk metadata\n",
    "print(hdb_parquet.metadata.row_group(0).column(9).statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd4a5afc-7db5-44b3-86ee-f91301126289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jorge</td>\n",
       "      <td>30</td>\n",
       "      <td>Developer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>32</td>\n",
       "      <td>Developer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  age        job\n",
       "0  Jorge   30  Developer\n",
       "1    Bob   32  Developer"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Spark to Pandas\n",
    "import pandas as pd\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "pandas_df = df.toPandas()\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "183e5971-92d5-403b-8153-67eeca032a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+---------+\n",
      "| name|age|      job|\n",
      "+-----+---+---------+\n",
      "|Jorge| 30|Developer|\n",
      "|  Bob| 32|Developer|\n",
      "+-----+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "spark_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
