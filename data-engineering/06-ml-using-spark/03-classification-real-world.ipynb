{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eab51edb-d784-43c5-9b0d-aadc60933764",
   "metadata": {},
   "source": [
    "## Classification Model\n",
    "\n",
    "### Task\n",
    "Dataset: Bank Marketing Dataset (available on Kaggle)\n",
    "Description: This dataset contains information about a bank's marketing campaigns, including customer data, campaign details, and response outcomes.\n",
    "Task: Predict whether a customer will subscribe to a term deposit based on their characteristics and campaign information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b75b09a-a600-4024-9935-1e5a5c3d3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --ignore-install -q pyspark\n",
    "!pip install --ignore-install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97d80ae-bebd-49d8-903e-dbcccbe5ebd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "base_path = os.getcwd()\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.appName('Bank-Marketing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "402cb746-c53f-4139-a4f4-506f02927979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- marital: string (nullable = true)\n",
      " |-- education: string (nullable = true)\n",
      " |-- default: string (nullable = true)\n",
      " |-- balance: integer (nullable = true)\n",
      " |-- housing: string (nullable = true)\n",
      " |-- loan: string (nullable = true)\n",
      " |-- contact: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- campaign: integer (nullable = true)\n",
      " |-- pdays: integer (nullable = true)\n",
      " |-- previous: integer (nullable = true)\n",
      " |-- poutcome: string (nullable = true)\n",
      " |-- deposit: string (nullable = true)\n",
      "\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "|age|        job| marital|education|default|balance|housing|loan|contact|day|month|duration|campaign|pdays|previous|poutcome|deposit|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "| 59|     admin.| married|secondary|     no|   2343|    yes|  no|unknown|  5|  may|    1042|       1|   -1|       0| unknown|    yes|\n",
      "| 56|     admin.| married|secondary|     no|     45|     no|  no|unknown|  5|  may|    1467|       1|   -1|       0| unknown|    yes|\n",
      "| 41| technician| married|secondary|     no|   1270|    yes|  no|unknown|  5|  may|    1389|       1|   -1|       0| unknown|    yes|\n",
      "| 55|   services| married|secondary|     no|   2476|    yes|  no|unknown|  5|  may|     579|       1|   -1|       0| unknown|    yes|\n",
      "| 54|     admin.| married| tertiary|     no|    184|     no|  no|unknown|  5|  may|     673|       2|   -1|       0| unknown|    yes|\n",
      "| 42| management|  single| tertiary|     no|      0|    yes| yes|unknown|  5|  may|     562|       2|   -1|       0| unknown|    yes|\n",
      "| 56| management| married| tertiary|     no|    830|    yes| yes|unknown|  6|  may|    1201|       1|   -1|       0| unknown|    yes|\n",
      "| 60|    retired|divorced|secondary|     no|    545|    yes|  no|unknown|  6|  may|    1030|       1|   -1|       0| unknown|    yes|\n",
      "| 37| technician| married|secondary|     no|      1|    yes|  no|unknown|  6|  may|     608|       1|   -1|       0| unknown|    yes|\n",
      "| 28|   services|  single|secondary|     no|   5090|    yes|  no|unknown|  6|  may|    1297|       3|   -1|       0| unknown|    yes|\n",
      "| 38|     admin.|  single|secondary|     no|    100|    yes|  no|unknown|  7|  may|     786|       1|   -1|       0| unknown|    yes|\n",
      "| 30|blue-collar| married|secondary|     no|    309|    yes|  no|unknown|  7|  may|    1574|       2|   -1|       0| unknown|    yes|\n",
      "| 29| management| married| tertiary|     no|    199|    yes| yes|unknown|  7|  may|    1689|       4|   -1|       0| unknown|    yes|\n",
      "| 46|blue-collar|  single| tertiary|     no|    460|    yes|  no|unknown|  7|  may|    1102|       2|   -1|       0| unknown|    yes|\n",
      "| 31| technician|  single| tertiary|     no|    703|    yes|  no|unknown|  8|  may|     943|       2|   -1|       0| unknown|    yes|\n",
      "| 35| management|divorced| tertiary|     no|   3837|    yes|  no|unknown|  8|  may|    1084|       1|   -1|       0| unknown|    yes|\n",
      "| 32|blue-collar|  single|  primary|     no|    611|    yes|  no|unknown|  8|  may|     541|       3|   -1|       0| unknown|    yes|\n",
      "| 49|   services| married|secondary|     no|     -8|    yes|  no|unknown|  8|  may|    1119|       1|   -1|       0| unknown|    yes|\n",
      "| 41|     admin.| married|secondary|     no|     55|    yes|  no|unknown|  8|  may|    1120|       2|   -1|       0| unknown|    yes|\n",
      "| 49|     admin.|divorced|secondary|     no|    168|    yes| yes|unknown|  8|  may|     513|       1|   -1|       0| unknown|    yes|\n",
      "+---+-----------+--------+---------+-------+-------+-------+----+-------+---+-----+--------+--------+-----+--------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(base_path + \"/work/06-ml-using-spark/bank.csv\",inferSchema=True,header=True)\n",
    "# Print the Schema of the dataframe\n",
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3b47772-9b53-40ac-816d-4da8a736c79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'job',\n",
       " 'marital',\n",
       " 'education',\n",
       " 'default',\n",
       " 'balance',\n",
       " 'housing',\n",
       " 'loan',\n",
       " 'contact',\n",
       " 'day',\n",
       " 'month',\n",
       " 'duration',\n",
       " 'campaign',\n",
       " 'pdays',\n",
       " 'previous',\n",
       " 'poutcome',\n",
       " 'deposit']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147b842",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "- Index Categorical Variables Including the Label\n",
    "- Assemble Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ce8a8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous', 'job_index', 'marital_index', 'education_index', 'default_index', 'housing_index', 'loan_index', 'contact_index', 'month_index', 'poutcome_index']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# List of categorical columns\n",
    "categorical_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month', 'poutcome']\n",
    "\n",
    "# Index categorical features\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column + \"_index\") for column in categorical_cols]\n",
    "\n",
    "# Index the label column\n",
    "label_indexer = StringIndexer(inputCol=\"deposit\", outputCol=\"label\")\n",
    "indexers.append(label_indexer)\n",
    "\n",
    "# List of input columns for the assembler\n",
    "assembler_inputs = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous'] + [column + \"_index\" for column in categorical_cols]\n",
    "print(assembler_inputs)\n",
    "\n",
    "# Create the assembler\n",
    "assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ff2d46",
   "metadata": {},
   "source": [
    "### Create and Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "621db674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(16,[0,1,2,3,4,5,...|  1.0|\n",
      "|(16,[0,1,2,3,4,5,...|  1.0|\n",
      "|(16,[0,1,2,3,4,5,...|  1.0|\n",
      "|(16,[0,1,2,3,4,5,...|  1.0|\n",
      "|(16,[0,1,2,3,4,5,...|  1.0|\n",
      "|[42.0,0.0,5.0,562...|  1.0|\n",
      "|[56.0,830.0,6.0,1...|  1.0|\n",
      "|[60.0,545.0,6.0,1...|  1.0|\n",
      "|(16,[0,1,2,3,4,5,...|  1.0|\n",
      "|[28.0,5090.0,6.0,...|  1.0|\n",
      "|[38.0,100.0,7.0,7...|  1.0|\n",
      "|(16,[0,1,2,3,4,5,...|  1.0|\n",
      "|[29.0,199.0,7.0,1...|  1.0|\n",
      "|[46.0,460.0,7.0,1...|  1.0|\n",
      "|[31.0,703.0,8.0,9...|  1.0|\n",
      "|[35.0,3837.0,8.0,...|  1.0|\n",
      "|[32.0,611.0,8.0,5...|  1.0|\n",
      "|(16,[0,1,2,3,4,5,...|  1.0|\n",
      "|(16,[0,1,2,3,4,5,...|  1.0|\n",
      "|[49.0,168.0,8.0,5...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [assembler])\n",
    "pipeline_model = pipeline.fit(data)\n",
    "data_transformed = pipeline_model.transform(data)\n",
    "data_transformed.select(\"features\", \"label\").show()\n",
    "\n",
    "pipeline_model.save(base_path + '/work/06-ml-using-spark/models/data_transformation_pipeline')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4534a",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "1. Split the Data into Training and Test Sets\n",
    "2. Train a Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4afed82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = data_transformed.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Train the model using the training data\n",
    "lr_model = lr.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9c9830",
   "metadata": {},
   "source": [
    "### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee09c612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.78045155772234...|\n",
      "|  1.0|       0.0|[0.68340248372662...|\n",
      "|  1.0|       0.0|[0.74065751071030...|\n",
      "|  1.0|       1.0|[0.24912785618237...|\n",
      "|  1.0|       1.0|[0.38382171928105...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = lr_model.transform(test_data)\n",
    "\n",
    "# View sample predictions\n",
    "predictions.select('label', 'prediction', 'probability').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552ac35",
   "metadata": {},
   "source": [
    "### Evaluate the Model's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bf70c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.8653\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize the evaluator\n",
    "evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol='rawPrediction', metricName='areaUnderROC')\n",
    "\n",
    "# Evaluate the model's performance\n",
    "auc = evaluator.evaluate(predictions)\n",
    "print(f\"Test Area Under ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e46cd2",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fe79c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the logistic regression model\n",
    "lr_model.save(base_path + \"/work/06-ml-using-spark/models/logistic_regression_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c74bf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "path_to_models = base_path + '/work/06-ml-using-spark/models/'\n",
    "new_pipeline_model = PipelineModel.load( path_to_models + 'data_transformation_pipeline')\n",
    "new_lr_model = LogisticRegressionModel.load(path_to_models + 'logistic_regression_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd650a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  0.0|       0.0|[0.78045155772234...|\n",
      "|  1.0|       0.0|[0.68340248372662...|\n",
      "|  1.0|       0.0|[0.74065751071030...|\n",
      "|  1.0|       1.0|[0.24912785618237...|\n",
      "|  1.0|       1.0|[0.38382171928105...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_data_transformed = new_pipeline_model.transform(data)\n",
    "# new_data_transformed.select(\"features\", \"label\").show()\n",
    "new_train_data, new_test_data = new_data_transformed.randomSplit([0.7, 0.3], seed=42)\n",
    "\n",
    "new_predictions = new_lr_model.transform(new_test_data)\n",
    "new_predictions.select('label', 'prediction', 'probability').show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
